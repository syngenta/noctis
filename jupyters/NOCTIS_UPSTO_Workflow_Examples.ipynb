{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# ‚ãÜ‚≠íÀö.‚ãÜUSPTO Dataset Preprocessing and Neo4j Import with Noctis üåô‚ãÜ‚≠íÀö.‚ãÜ\n",
    "\n",
    "This Jupyter notebook is your go-to resource for navigating the complete workflow for the USPTO dataset. It walks you through converting the data into a format ready for graph ingestion using Noctis and importing it into a Neo4j database. You'll find examples of querying the database with built-in queries, along with guidance on crafting and executing your own custom queries, all designed to help you efficiently explore and interact with the patent data in a graph-based setup.\n"
   ],
   "id": "2fcc172b7a0facb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Table of Contents\n",
    "0Ô∏è‚É£ üì• Downloading & unpacking the USPTO dataset\n",
    "\n",
    "1Ô∏è‚É£ üèóÔ∏è Defining a graph schema with Noctis\n",
    "\n",
    "2Ô∏è‚É£ üì¶ Bulk data ingestion (CSV) into Neo4j\n",
    "\n",
    "3Ô∏è‚É£ üîÑ Incremental update into Neo4j\n",
    "\n",
    "4Ô∏è‚É£ üîç Querying the graph database"
   ],
   "id": "4a604a68431b2606"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0. üì• Download USPTO dataset\n",
    "This is a preparatory step to load the data for this example exercise\n",
    "\n"
   ],
   "id": "ec036bd0ad67ec7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "# Link to USPTO dataset\n",
    "\n",
    "DATA_URL = 'https://github.com/wengong-jin/nips17-rexgen/blob/master/USPTO/data.zip?raw=true'\n",
    "\n",
    "# These are folders where the data will be downloaded and processed\n",
    "# Adjust to your liking\n",
    "\n",
    "RAW_DIR = './data/raw'\n",
    "PROCESSED_DIR = './data/processed'\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# File names which are going to be used throughout this notebook \n",
    "\n",
    "USPTO_FILE = 'uspto.csv'\n",
    "\n",
    "# First five lines of USPTO for quick checks\n",
    "\n",
    "FIVELINES_FILE = 'uspto_5lines.csv'\n",
    "\n",
    "NOCTIS_HEADER = ['ChemicalEquation.smiles', 'ChemicalEquation.reaction_center']"
   ],
   "id": "3187571cf9d8ed17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download and extract\n",
    "\n",
    "zip_path = os.path.join(RAW_DIR, 'uspto_data.zip')\n",
    "urllib.request.urlretrieve(DATA_URL, zip_path)\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    z.extractall(RAW_DIR)\n",
    "\n",
    "# Combine splits\n",
    "\n",
    "files = ['train.txt', 'test.txt', 'valid.txt']\n",
    "RAWDATA_DIR = os.path.join(RAW_DIR, 'data')\n",
    "\n",
    "combined = pd.concat(\n",
    "    [pd.read_csv(os.path.join(RAWDATA_DIR, f), sep=' ', header=None, names=NOCTIS_HEADER) for f in files],\n",
    "    ignore_index=True\n",
    ")\n",
    "combined.to_csv(os.path.join(PROCESSED_DIR, USPTO_FILE), index=False)"
   ],
   "id": "1572d6f8a499d0a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fa16ce43687e4af",
   "metadata": {},
   "source": [
    "# Read the first 5 lines of the CSV file\n",
    "\n",
    "df = pd.read_csv(os.path.join(PROCESSED_DIR, USPTO_FILE), delimiter=',', nrows=5)\n",
    "\n",
    "# Write the first 5 lines to a new CSV file\n",
    "\n",
    "df.to_csv(os.path.join(PROCESSED_DIR, FIVELINES_FILE), sep=',', lineterminator='\\n', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. üèóÔ∏è Defining a graph schema with Noctis\n",
    "Define node & relationship types either from a dictionary, YAML or JSON file."
   ],
   "id": "82cdf140fa0614cc"
  },
  {
   "cell_type": "code",
   "id": "a8e0aec36ed136cc",
   "metadata": {},
   "source": "from noctis.data_architecture.graph_schema import GraphSchema",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gs = GraphSchema.build_from_dict({'extra_nodes':{'molecule':'ExtraMolecule', 'chemical_equation':'ExtraChemicalEquation'}})\n",
    "\n",
    "#gs = GraphSchema.build_from_file(file_path = 'schema.yaml', file_format = 'yaml')"
   ],
   "id": "205fc40b5605e0ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## The graph schema can aslo be saved to YAML or JSON\n",
    "\n",
    "gs.save_to_file('schema_saved.yaml', file_format='yaml')"
   ],
   "id": "f63bf13abd70418d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(gs)",
   "id": "345f6a79dcfce199",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. üì¶ Bulk data ingestion (CSV) into Neo4j",
   "id": "56ed68dfa6ab5320"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### A) Preprocessing\n",
    "How to prepare large volumes of data for Neo4j ingestion"
   ],
   "id": "bfb6f0fda68c2042"
  },
  {
   "cell_type": "code",
   "id": "11c468682a8c0b9c",
   "metadata": {},
   "source": "from noctis.data_transformation.preprocessing.data_preprocessing import Preprocessor\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "Preprocessor.info()",
   "id": "26022a22aca8fbf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68c93b0fc6787e28",
   "metadata": {},
   "source": [
    "# To initialise Preprocessor with default graph schema\n",
    "\n",
    "preprocessor = Preprocessor() \n",
    "\n",
    "# To initialise Preprocessor with custom schema\n",
    "\n",
    "preprocessor = Preprocessor(schema = gs) \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Preprocessor configurations can be loaded from a YAML file \n",
    "\n",
    "preprocessor.set_config_from_yaml(file_path = 'preprocessor_config.yaml')"
   ],
   "id": "5066ee5f2357253e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# To make sure that CSV file is interpreted correctly, follow the suggested header format\n",
    "# In the case of USPTO, the header looks like this\n",
    "\n",
    "print(NOCTIS_HEADER)\n",
    "\n",
    "# which means that each ChemicalEquation node will have a property reaction_center "
   ],
   "id": "33902bab2c223799",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e172f0a39c114dbe",
   "metadata": {},
   "source": [
    "INPUT_FILE = os.path.join(PROCESSED_DIR, FIVELINES_FILE)\n",
    "\n",
    "# Preprocessing can be done in parallel\n",
    "\n",
    "preprocessor.preprocess_csv_for_neo4j_parallel(input_file=INPUT_FILE, validation = True, output_folder='../output_uspto', prefix='USPTO',delimiter=',', blocksize=1000 )\n",
    "# We offer an option to validate reaction strings, to ensure consistency of the data and its correct transformation into a graph \n",
    "# You can also turn it off at your own risk..\n",
    "\n",
    "# Users have the option to provide their own Dask Client instance for parallel processing.\n",
    "# Example usage: preprocessor.preprocess_csv_for_neo4j_parallel(input_file=INPUT_FILE, dask_client=Client(...))\n",
    "# Note: If users supply their own Dask Client, they must also manage closing the connection themselves.\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Alternatively, it's also possible to process files in serial mode:\n",
    "# preprocessor.preprocess_csv_for_neo4j_serial(input_file='./data/processed/uspto_5lines.csv', output_folder='../output_test', prefix='USPTO',delimiter=',', chunksize=5 )\n",
    "# where chunksize defines how many rows in one partition the preprocessor is going to handle.\n",
    "# In serial mode it is also possible to define nrows, which will limit the preprocessing to the first n rows. \n",
    "# If chunksize is not defined, the whole file will be processed in one go. "
   ],
   "id": "df3548e5ec1ab421",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# If some reactions fail, or if some rows have empty reaction strings, they can be found in corresponding files for further investigation",
   "id": "fddfe1cf7ae23dd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Both graph schema and configurations can be saved into files, so it can be used to reproduce the preprocessing.\n",
    "\n",
    "preprocessor.schema.save_to_file(file_path ='schema_saved.yaml', file_format ='yaml')\n",
    "preprocessor.config.save_to_yaml(file_path = './config_saved.yaml')"
   ],
   "id": "36281b44149edbd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### B) Importing\n",
    "Now that we've preprocessed our data, we're ready to establish a connection with Neo4j DB and import our prepared datasets into the graph database.\n",
    "<br> We assume that you have already Neo4j DB instance configured and started.\n"
   ],
   "id": "7683ea49e903d693"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from noctis import settings\n",
    "\n",
    "# IMPORTANT: Use secret management system to store your secret information\n",
    "# such as database URIs, usernames, and password. Never hardcode these values in your scripts.\n",
    "# Repository by default is instantiated with credentials defined in settings. \n",
    "# If you configured your setting, no need to pass the attributes\n",
    "\n",
    "URI = settings.NEO4J_DEV_URL\n",
    "USERNAME = settings.NEO4J_DEV_USER # default username in Neo4j is \"neo4j\". If you didn't define another name explicitly through Neo4j Desktop, your username is \"neo4j\"\n",
    "PASSWORD = settings.NEO4J_DEV_PASSWORD\n",
    "\n",
    "print(URI, USERNAME, PASSWORD)\n"
   ],
   "id": "68e3c0bc9846f4b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from noctis.repository.neo4j.neo4j_repository import Neo4jRepository\n",
    "\n",
    "# Repository takes as an input GraphSchema object to dynamically define correct node labels and relationships types\n",
    "# If you want to use default GraphSchema, you don't have to pass it to the repository\n",
    "\n",
    "# The default database in Neo4j is 'neo4j'\n",
    "repo = Neo4jRepository(database='uspto', schema = gs)\n",
    "\n",
    "# With credentials not from settings\n",
    "# repo = Neo4jRepository(uri='bolt://localhost:0000', username=\"neo4j\", password='123123123', database='uspto')"
   ],
   "id": "349344f5ac0857d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Before importing data, create uniqueness constraints\n",
    "# They are created automatically for all nodes defined in GraphSchema based on UID\n",
    "\n",
    "repo.create_constraints()"
   ],
   "id": "6176fb1b4f5ec295",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "repo.show_constraints()",
   "id": "e24e0fb07167c106",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "IMPORT_PATH = <YOUR IMPORT PATH>\n",
    "# If your Neo4j is set up to have an import directory,\n",
    "# first move the files to the import directory before running this query and remove folder_path attribute\n",
    "# If your Neo4j is set up to import files from anywhere in your file system, define the import path\n",
    "\n",
    "repo.execute_query(query_name = 'import_db_from_csv', folder_path = IMPORT_PATH, prefix = 'USPTO')\n",
    "# Now your reaction data is graphormed! "
   ],
   "id": "9a8afec3c4209f76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# You can see that the graph schema in Neo4j GDB is the same as you defined in the beginning\n",
    "\n",
    "repo.execute_query('get_gdb_schema')"
   ],
   "id": "b784300ce5362d4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. üîÑ Incremental update into Neo4j",
   "id": "85dbc4defd518bb2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### A) Preprocessing\n",
    "Reaction data can be added to the graph not only from a CSV file, but also from a Python Object. This is suggested for small volumes of data"
   ],
   "id": "93f5388b7b64ff0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's see how reactions from a list of reaction strings can be added to the GDB\n",
    "\n",
    "my_list_of_strings = ['N#Cc1ccsc1N.O=[N+]([O-])c1cc(F)c(F)cc1F>>N#Cc1ccsc1Nc1cc(F)c(F)cc1[N+](=O)[O-]','O=Cc1cncc(Cl)c1COC1CCCCO1>>OCc1cncc(Cl)c1COC1CCCCO1', 'NC1CCN(CC2Cn3c(=O)ccc4ncc(F)c2c43)CC1O.O=Cc1cc2c(cn1)OCS2>>Cl.O=c1ccc2ncc(F)c3c2n1CC3CN1CCC(NCc2cc3c(cn2)OCS3)C(O)C1', 'WRONG>>STRING']"
   ],
   "id": "f153e8af98c9a2eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# When data is loaded from a list of reactions, only base nodes and relationships can be generated\n",
    "\n",
    "preprocessor = Preprocessor()"
   ],
   "id": "9bb93957d26343e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This creates a DataContainer class -- an internal data class of Noctis\n",
    "\n",
    "reaction_data = preprocessor.preprocess_object_for_neo4j(data = my_list_of_strings , data_type = 'reaction_string')"
   ],
   "id": "7c784db115022a99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If some of the reactions will not be processed, they can be fetched from preprocessor\n",
    "\n",
    "preprocessor.get_failed_strings()"
   ],
   "id": "1a236399a2247ad4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### B) Merging\n",
    "DataContainer object can be merged to the Graph. If generated nodes don't exist in the Graph, they will be created, but if they exist already, they will not be added/duplicated."
   ],
   "id": "3ff35b2464fa3f69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "repo.execute_query(query_name = 'load_nodes_and_relationships', data = reaction_data, data_type = 'data_container')\n",
   "id": "59995f778f80c215",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Python objects could be loaded also directly through this query, but in this way you won't be able to access failed reaction strings\n",
    "\n",
    "repo.execute_query(query_name = 'load_nodes_and_relationships', data = my_list_of_strings, data_type = 'reaction_string')"
   ],
   "id": "989b1ebf26af384c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. üîç Querying the graph database\n",
    "All the data is in the graph! It's time to run some queries\n",
    "<br> We implemented some built-in queries but also left an option to run easily your own queries\n",
    "<Br> First, about the hard-coded queries:"
   ],
   "id": "160130889fa35223"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# All the available queries and their input parameters are listed in the helper functino of Neo4jRepository class\n",
    "\n",
    "Neo4jRepository.info()"
   ],
   "id": "f44462567206aad1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Each query depending on its type can return either an unprocessed Neo4j output (modify_graph), or a pandas DataFrame (retrieve_stats), or a DataContainer (retrieve_graph)\n",
    "# Let's try running a retrieve_graph type of query\n",
    "\n",
    "node = repo.execute_query(query_name = 'get_node',node_uid=\"M100203740101976085840472731949187371946\" )"
   ],
   "id": "2b4d925525d574e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "\n",
    "# Users can define their own queries in a yaml, which has a very simple format.\n",
    "# Each query should have query_name, query_type and query itself\n",
    "# Cypher query can be juts copy-pasted into the YAML\n",
    "# Query types are the same as for built-in queries: modify_graph, retrieve_stats, retrieve_graph\n",
    "# If it requires arguments, it's also possible to define query_args_required and query_args_optional\n",
    "\n",
    "CUSTOM_QUERY_YAML = './queries/custom_query.yaml'\n",
    "\n",
    "with open(CUSTOM_QUERY_YAML, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "    print(yaml.dump(yaml_data, default_flow_style=False))"
   ],
   "id": "d0a0e711f7e9cf0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This is how you can execute a custom query\n",
    "\n",
    "node_by_smiles = repo.execute_custom_query_from_yaml(yaml_file='queries/custom_query.yaml', query_name='get_node_by_smiles', smiles ='COc1ccc(CN(Cc2ccc(OC)cc2)c2nc(C)nc(-c3cc(CN4CCN(S(C)(=O)=O)CC4C)cnc3Nc3cnc(OC)c(F)c3)n2)cc1')\n",
    "print(node_by_smiles)"
   ],
   "id": "1e6a6ee3e3c40a26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This custom query retrieve_stats as pandas.DataFrame\n",
    "\n",
    "repo.execute_custom_query_from_yaml(yaml_file='queries/custom_query.yaml', query_name='count_nodes')"
   ],
   "id": "c8172eacff11166d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The queries which return Noctis DataContainer can be transformed in other python objects\n",
    "# Let's retrieve some routes for this molecule\n",
    "\n",
    "routes = repo.execute_query('get_routes', root_node_uid=\"M100203740101976085840472731949187371946\", max_number_reactions=7)"
   ],
   "id": "e2be7a2641e8930b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(routes)",
   "id": "e1e4d390477c8740",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DataContainer info method has a short example of what can be done with DataContainer\n",
    "# and list of available transformations\n",
    "\n",
    "routes.info()"
   ],
   "id": "f1f625bf1af6d3d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Noctis DataContainer can be transformed into list of reaction strings, netowrkX objects, or pandas DataFrames\n",
    "# When transformed to pandas, two data frames are returned:\n",
    "# One contains Nodes and the other Relationships. \n",
    "# The record_id is used to maintain the association between nodes and relationships that are part of a single result record \n",
    "# in a query output\n",
    "\n",
    "routes.transform_to(format_type='pandas')[0]"
   ],
   "id": "ecb2bbc4c2a455e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "routes.transform_to(format_type='pandas')[1]",
   "id": "e775b469c8fc9c4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "be671e40e7fb795a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
